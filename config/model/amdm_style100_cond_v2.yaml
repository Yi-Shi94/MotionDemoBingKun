model_name: "AMDMCOND"

diffusion:
    sample_mode: "ddpm"
    estimate_mode: "epsilon"
    loss_type: "l2"
    noise_schedule_mode: "cosine"
    T: 50
    #T_inference: 50

model_hyperparam:
    layer_num: 10
    #cond_mask_prob: 0
    #cond_emb_size: 256 #condition frame for [class, text, etc]
    norm_type: "group_norm"
    time_emb_size: 256 #time embediing
    hidden_size: 1024 
    use_cond: True
    selected_style: "ALL"


optimizer:
    anneal_times: 5
    initial_lr: 0.0001
    final_lr: 0.00001
    teacher_epochs: 200
    ramping_epochs: 30
    student_epochs: 20
    mini_batch_size: 128
    rollout: 3
    EMA: 
        ema_decay: 0.9999
        ema_start: 5000
        ema_update_rate: 1

data:
    clip_model_path: "./deps/clip-vit-large-patch14"   
    dataset_name: "STYLE100"
    dataset_class_name: "STYLE100CLIP"
    path: "data/100STYLE"
    min_motion_len: 0
    max_motion_len: -1
    data_trim_begin: 0
    data_trim_end: 0
    fps: 30
    use_angle: False
    use_vel: True
    
test:
    test_interval: 10
    test_num_steps: 60
    test_num_trials: 8
    test_num_init_frame: 5
